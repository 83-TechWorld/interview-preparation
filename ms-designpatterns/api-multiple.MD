```markdown
# ğŸ›¡ï¸ Defending APIs from Abuse: Rate Limiting Done Right

## Problem Statement

> **If a hacker spams your API, how do you stop themâ€”without crashing yourself?**

A naive rate-limiting approach looks like this:

1. Request arrives  
2. Query database â†’ â€œHow many requests has this user made?â€  
3. Decide allow / deny  

âŒ **This is dangerous**

- Every request now triggers **DB reads**
- An attacker can intentionally flood requests
- The **rate-limit check itself becomes the Denial-of-Service attack**
- Your database dies before your business logic is even touched

This is a classic **self-inflicted DoS**

---

## Core Requirement for a Real Solution

A production-grade rate-limiter **must**:

- âœ… Run in **O(1) constant time**
- âœ… Use **no disk I/O**
- âœ… Work entirely **in memory**
- âœ… Be **predictable under abuse**
- âœ… Scale across multiple servers / regions

Thatâ€™s exactly why modern systems use **Token Bucket**

---

## âœ… The Solution: Token Bucket Algorithm


::contentReference[oaicite:0]{index=0}


### Mental Model ğŸª£

- Imagine a bucket with **capacity = 10 tokens**
- Tokens are added at a fixed rate (e.g. **2 tokens/sec**)
- **Each request consumes 1 token**

### Decision Path (Critical Simplicity)

```

Request arrives
â†“
Is token available?
â”œâ”€ Yes â†’ consume token â†’ allow request
â””â”€ No  â†’ reject immediately (HTTP 429)

```

No queues  
No waiting  
No retries  
No cascading failures  

---

## Where Does the State Live?

**Important detail most explanations skip**

Token Bucket is enforced **per key**, not globally.

### Rate-Limit Key Design

```

rate_limit:{scope}:{identifier}:{resource}

```

Example:
```

rate_limit:user:42:POST:/v1/orders

````

This gives you:
- Per-user limits
- Per-API limits
- Per-method limits
- Per-IP limits (for anonymous traffic)

Each key owns **its own bucket**

---

## What Is Stored (Only 2 Values!)

For each key, store **only**:

```text
current_tokens
last_updated_timestamp
````

Usually stored in **Redis**

No counters
No logs
No history tables

---

## Lazy Refill (No Background Jobs!)

There is **no token refiller thread**

Instead, tokens are calculated **only when a request arrives**

```text
elapsed_time = now - last_updated_timestamp
new_tokens   = elapsed_time * refill_rate
current_tokens = min(capacity, current_tokens + new_tokens)
```

Then:

* If `current_tokens >= 1` â†’ allow & decrement
* Else â†’ reject immediately

---

## Atomicity (Why Lua Scripts Are Used)

All of this logic must be **atomic**

Otherwise:

* Two concurrent requests might both pass
* Limits become meaningless under load

### Standard Approach

* Use **Redis Lua scripts**
* Entire calculation + update runs as **one atomic operation**

This is why Redis is the industry default for rate limiting

---

## Why This Scales So Well

| Property           | Result                  |
| ------------------ | ----------------------- |
| In-memory only     | No DB bottlenecks       |
| Constant time      | Predictable latency     |
| Stateless services | Easy horizontal scaling |
| Deterministic      | Safe under attack       |
| Edge-friendly      | Works at gateways/CDNs  |

---

## How Big Companies Use This

* **Stripe**
  Per-API-key token buckets to protect payment endpoints

* **Shopify**
  Per-store and per-app rate buckets

* **Cloudflare**
  Token buckets enforced at the **edge**, before traffic hits origin

If your system survives traffic spikes gracefully, a token bucket is almost certainly involved.

---

## Other Rate-Limiting Algorithms (When to Use What)

### 1ï¸âƒ£ Fixed Window Counter âŒ

**How it works**

* Count requests per minute

**Problems**

* Burst at window boundary
* Easy to exploit

**Use only for**

* Simple dashboards
* Non-critical internal tools

---

### 2ï¸âƒ£ Sliding Window Log âŒ

**How it works**

* Store timestamps of all requests

**Problems**

* High memory usage
* Cleanup complexity

---

### 3ï¸âƒ£ Sliding Window Counter âš ï¸

**How it works**

* Approximation using weighted windows

**Better than fixed**

* Still less precise than Token Bucket

---

### 4ï¸âƒ£ Leaky Bucket âš ï¸

**How it works**

* Requests are queued and drained at fixed rate

**Problems**

* Queues build up
* Increases latency under load

---

### âœ… 5ï¸âƒ£ Token Bucket (Best Default)

**Why it wins**

* Allows bursts
* No queues
* Simple math
* Industry standard

---

## Libraries & Tools You Can Use

### JVM / Spring Boot

* **Bucket4j**
* **Resilience4j RateLimiter**
* **Spring Cloud Gateway (RedisRateLimiter)**

### Node.js

* `rate-limiter-flexible`
* `express-rate-limit` (Redis-backed)

### Go

* `golang.org/x/time/rate`

### Infrastructure / Edge

* **NGINX** (limit_req_zone)
* **Envoy**
* **Kong**
* **AWS API Gateway throttling**

---

## Advanced Real-World Patterns

### ğŸ”¹ Multi-Level Rate Limiting

Apply limits at:

* IP level
* User level
* API key level
* Endpoint level

### ğŸ”¹ Fail-Closed Strategy

If Redis is down:

* Default to **reject**
* Protect system integrity

### ğŸ”¹ Separate Buckets for Reads vs Writes

Writes are expensive â†’ tighter limits

---

## Final Takeaway

> **Rate limiting is not about fairness â€” itâ€™s about survival**

* Databases should never be in the rate-limit path
* Counters are dangerous under attack
* Token Bucket is fast, deterministic, and battle-tested

If your system stays up during traffic storms,
a **tiny in-memory bucket** is probably doing the hardest job silently ğŸª£ğŸš€

---

```
```
