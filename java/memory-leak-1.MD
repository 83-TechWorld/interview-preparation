---

# 1ï¸âƒ£ ThreadLocal Memory Leak (MOST COMMON & DANGEROUS)

---

## ğŸ”¹ What is `ThreadLocal` (simple words)

`ThreadLocal` stores data **per thread**, not per request.

Used for:

* User context
* Transaction ID
* Locale
* Security context

```java
ThreadLocal<UserContext> context = new ThreadLocal<>();
```

---

## âŒ How Memory Leak Happens

### âŒ BAD CODE (Very common)

```java
public class UserContextHolder {
    public static ThreadLocal<String> userContext = new ThreadLocal<>();
}
```

```java
public void processRequest() {
    UserContextHolder.userContext.set("USER_123");
    // business logic
}
```

### ğŸš¨ Whatâ€™s wrong?

* Value is **set**
* Value is **never removed**
* Thread lives long (Tomcat / thread pool)
* Next request â†’ same thread reused
* Old object stays forever

ğŸ‘‰ **ThreadLocal + Thread Pool = MEMORY LEAK**

---

## ğŸ” How You Detect This (VisualVM + MAT)

### In VisualVM

* Heap keeps growing
* GC runs but memory doesnâ€™t drop
* `ThreadLocalMap` objects keep increasing

### In Eclipse MAT

* Leak Suspects Report shows:

```
java.lang.ThreadLocal$ThreadLocalMap$Entry
```

* Path to GC Root:

```
Thread â†’ ThreadLocalMap â†’ YourObject
```

---

## âœ… Correct Fix (MANDATORY)

```java
try {
    UserContextHolder.userContext.set("USER_123");
    // business logic
} finally {
    UserContextHolder.userContext.remove(); // VERY IMPORTANT
}
```

---

## ğŸ”‘ Interview Line

> â€œThreadLocal leaks happen because application servers reuse threads. Always call `remove()`.â€

---

# 2ï¸âƒ£ Spring Boot â€“ REAL PRODUCTION MEMORY LEAK CASE

---

## ğŸ¯ Real Scenario: Cache Gone Wrong

### âŒ BAD CODE (Seen in many projects)

```java
@Service
public class UserCacheService {

    private static Map<Long, User> cache = new HashMap<>();

    public User getUser(Long id) {
        return cache.computeIfAbsent(id, this::loadFromDB);
    }
}
```

---

## âŒ Whatâ€™s wrong?

* Static cache
* No eviction
* Users keep increasing
* Heap grows until **OutOfMemoryError**

---

## ğŸ” How It Looks in Production

### Symptoms

* App becomes slow after hours/days
* Full GC spikes
* Eventually crashes

### VisualVM

* Heap usage keeps increasing
* `HashMap`, `User` objects dominate heap

### Eclipse MAT

* Dominator Tree:

```
UserCacheService.cache
 â””â”€â”€ HashMap
     â””â”€â”€ User
```

* Retained Heap = huge

---

## âœ… Correct Production Fix

### âœ”ï¸ Use a real cache with eviction

```java
Cache<Long, User> cache = Caffeine.newBuilder()
    .maximumSize(10_000)
    .expireAfterWrite(10, TimeUnit.MINUTES)
    .build();
```

Or Redis for distributed systems.

---

## ğŸ”‘ Interview Line

> â€œNever use static HashMaps as caches in Spring Boot. Use bounded caches with eviction.â€

---

# 3ï¸âƒ£ How Netflix / Amazon Detect Memory Leaks in Production

This is **very important** and **very practical**.

---

## ğŸ—ï¸ Production Reality

* You **cannot attach VisualVM** to prod JVM
* You **cannot restart often**
* Traffic is massive

So they use **proactive monitoring**

---

## ğŸ§  Strategy 1: JVM Metrics (Always ON)

### Metrics Collected

* Heap usage
* Old Gen usage
* GC pause time
* Allocation rate

### Tools Used

* Micrometer
* Prometheus
* Grafana

### Red Flag

> Old Gen steadily rising over time â†’ MEMORY LEAK

---

## ğŸ§  Strategy 2: Canary & Baseline Comparison

Netflix compares:

* **Old instance** vs **New deployment**
* Memory slope over time

If:

```
New version memory â†‘ faster than baseline
```

ğŸ‘‰ Rollback immediately

---

## ğŸ§  Strategy 3: Heap Dump on Demand

When alert triggers:

* JVM automatically takes heap dump:

```bash
-XX:+HeapDumpOnOutOfMemoryError
```

Heap dump stored in S3
Analyzed offline using **Eclipse MAT**

---

## ğŸ§  Strategy 4: Allocation Rate Monitoring

Instead of waiting for OOM:

* Monitor **bytes allocated/sec**
* Sudden increase = suspicious code path

Netflix uses:

* Java Flight Recorder (JFR)
* Async Profiler

---

## ğŸ§  Strategy 5: Kill & Replace (Cloud Advantage)

If memory grows abnormally:

* Instance is killed
* Traffic rerouted
* Root cause analyzed later

ğŸ‘‰ **Design for failure**

---

## ğŸ§  Netflix / Amazon Golden Rule

> â€œInstances are cattle, not pets.â€

---

# ğŸ§ª Real Production Memory Leak Detection Flow

```text
Metrics Alert (Old Gen rising)
        â†“
Traffic shifted
        â†“
Heap dump captured
        â†“
MAT analysis
        â†“
Bug fixed
        â†“
Canary rollout
```

---

# ğŸ”¥ Summary (VERY IMPORTANT)

| Topic       | Key Lesson                          |
| ----------- | ----------------------------------- |
| ThreadLocal | Always call `remove()`              |
| Spring Boot | No unbounded static caches          |
| Production  | Detect via metrics, not debugging   |
| Tools       | VisualVM â†’ Detect, MAT â†’ Root cause |

---

# ğŸ¯ If You Want Next (Highly Recommended)

Great choice âœ…
**Connection Pool memory leaks** are **very common in real production systems** (Spring Boot, Hibernate, JDBC).
Letâ€™s go **step-by-step**, with **clear example â†’ symptoms â†’ detection â†’ tools â†’ fix**.

---

# âœ… Connection Pool Memory Leak (FULL EXPLANATION)

![Image](https://www.delftstack.com/img/Java/Connection%20pool%20with%20JDBC.jpg?utm_source=chatgpt.com)

![Image](https://groups.google.com/group/hikari-cp/attach/2b3094903cc0c/Screenshot%202022-02-17%20at%2010.11.16%20AM.png?part=0.1\&view=1\&utm_source=chatgpt.com)

![Image](https://avaldes.com/wp-content/uploads/2013/03/deadlock_screenshot.png?utm_source=chatgpt.com)

![Image](https://eclipse.dev/mat/images/mat_thumb.png?utm_source=chatgpt.com)

---

## 1ï¸âƒ£ What is a Connection Pool (simple)

A **connection pool** keeps a fixed number of DB connections open and **reuses** them.

Popular pools:

* **HikariCP** (default in Spring Boot)
* Apache DBCP
* c3p0

ğŸ‘‰ Creating DB connections is expensive, so we reuse them.

---

## 2ï¸âƒ£ What is a Connection Pool Leak?

A **connection leak** happens when:

* A connection is **borrowed**
* But **never returned** to the pool

Over time:

* Pool becomes empty
* New requests block
* Threads pile up
* App slows / crashes

---

## 3ï¸âƒ£ BAD CODE (Real Leak Example)

```java
public User getUser(Long id) throws SQLException {
    Connection conn = dataSource.getConnection();
    PreparedStatement ps =
        conn.prepareStatement("SELECT * FROM users WHERE id=?");

    ps.setLong(1, id);
    ResultSet rs = ps.executeQuery();

    if (rs.next()) {
        return mapUser(rs);
    }

    // âŒ connection NOT closed
}
```

### âŒ Whatâ€™s wrong?

* `Connection` never closed
* Pool loses 1 connection per request
* After N requests â†’ **POOL EXHAUSTED**

---

## 4ï¸âƒ£ Symptoms in Production

### ğŸš¨ Application Symptoms

* Requests hang
* Response time increases
* Eventually timeouts

### ğŸš¨ Logs (Very Common)

```
HikariPool-1 - Connection is not available, request timed out
```

---

## 5ï¸âƒ£ Why This Becomes a MEMORY LEAK

Even though it's a **resource leak**, it also causes a **memory leak** because:

* Blocked threads stay in memory
* Stack traces retained
* Request objects retained
* Heap usage increases

ğŸ‘‰ MAT will show:

```
java.lang.Thread
java.sql.Connection
```

retained for long time

---

## 6ï¸âƒ£ Detecting Leak with VisualVM (LIVE)

### Step 1: Attach VisualVM

* Attach to running JVM

### Step 2: Observe Threads

Look for:

* Many threads in **WAITING / BLOCKED**
* Stack traces pointing to:

```text
getConnection()
```

### Step 3: Memory Tab

* Heap slowly growing
* GC not freeing blocked request objects

---

## 7ï¸âƒ£ Detecting Leak with Eclipse MAT (ROOT CAUSE)

### Step 1: Capture Heap Dump

```bash
jmap -dump:format=b,file=heap.hprof <pid>
```

### Step 2: Open in MAT

* Leak Suspects Report

### Step 3: Dominator Tree

You will see:

```
Thread
 â””â”€â”€ Request
     â””â”€â”€ Connection
```

### Path to GC Root:

```
Thread â†’ Stack Frame â†’ Connection
```

âœ”ï¸ Confirms **connection not released**

---

## 8ï¸âƒ£ Correct Code (MUST FOLLOW)

### âœ… Always use try-with-resources

```java
public User getUser(Long id) {
    String sql = "SELECT * FROM users WHERE id=?";

    try (Connection conn = dataSource.getConnection();
         PreparedStatement ps = conn.prepareStatement(sql)) {

        ps.setLong(1, id);
        ResultSet rs = ps.executeQuery();

        if (rs.next()) {
            return mapUser(rs);
        }
    } catch (SQLException e) {
        throw new RuntimeException(e);
    }
    return null;
}
```

âœ”ï¸ Connection automatically returned to pool

---

## 9ï¸âƒ£ Hibernate / JPA Leak Example

### âŒ BAD CODE

```java
Session session = sessionFactory.openSession();
// no close()
```

### âœ… FIX

```java
try (Session session = sessionFactory.openSession()) {
   // work
}
```

Or let **Spring manage transactions**:

```java
@Transactional
public void saveUser(User user) {
   repository.save(user);
}
```

---

## ğŸ”¥ HikariCP â€“ Built-in Leak Detection (VERY IMPORTANT)

### Enable Leak Detection

```yaml
spring.datasource.hikari.leak-detection-threshold: 30000
```

â±ï¸ 30 seconds

### What Happens?

If a connection is held longer than 30s:

```
Connection leak detection triggered
```

âœ”ï¸ Stack trace shows **exact code line**

---

## 10ï¸âƒ£ Production Monitoring (Netflix / Amazon Style)

### Metrics to Monitor

* Active connections
* Idle connections
* Threads waiting for connection

### Tools Used

* Micrometer
* Prometheus
* Grafana

### Red Flag Pattern

```
Active connections â†‘
Idle connections â†“
Waiting threads â†‘
```

ğŸ‘‰ Connection leak confirmed

---

## 11ï¸âƒ£ Typical Real-World Causes

| Cause               | Example                        |
| ------------------- | ------------------------------ |
| Exception path      | Connection not closed on error |
| Async calls         | Connection used outside thread |
| Manual JDBC         | No try-with-resources          |
| Long transactions   | Holding connection too long    |
| Streaming ResultSet | Not fully consumed             |

---

## 12ï¸âƒ£ Interview-Ready Summary (IMPORTANT)

> â€œConnection pool leaks happen when connections are not returned to the pool.
> I detect them using Hikari leak detection, VisualVM thread analysis, and MAT heap dumps.
> The fix is always try-with-resources and proper transaction boundaries.â€

---

## ğŸ”„ Real Production Detection Flow

```text
Slow API
   â†“
Hikari timeout logs
   â†“
Check active vs idle connections
   â†“
Enable leak detection
   â†“
Fix code
```

---

]